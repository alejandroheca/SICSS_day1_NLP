---
title: "SICSS Day 1: NLP"
subtitle: "Word Embeddings: Exercise"
output:
  html_document:
    code_folding: hide
date: "`r format(Sys.time(), '%d %B, %Y')`"

author: "Alejandro Hermida Carrillo"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 5)
```

## Load Libraries

```{r, include=FALSE}

# Expression that loads a package by name
req <- substitute(require(x, character.only = TRUE))
# List of libraries
libs <- c("data.table", "ggplot2","tidyverse","conText","quanteda")
# Attempt to load each package; if not installed, install it and try again
sapply(libs, function(x) eval(req) || {install.packages(x, dependencies = TRUE); eval(req)})

```

## Set working directory, load helper files

```{r, include=FALSE}
# Set working directory    -----------------------------------------------------
path <- dirname(rstudioapi::getActiveDocumentContext()$path) # This is a relative path. Set alternative path if necessary
setwd(path)
getwd()

#Load the helper functions to work with simple embÂ§edding computations

source("helpers/SICSS_day1_helpers_functions.R")

```


## Excercise 2: How do Biden and Trump talk about each other?

### 2.1. Who talks more about the other in the speeches?

##### Let's start by loading our (clean) speeches dataset

```{r}

speeches_clean <- readRDS("SICSS_day1_NLP/data/speeches_clean.rds")

```

##### My solution: because we are comparing different words (Biden talks about Trump and Trump talks about Biden) we give both terms the same label.

```{r}

speeches_exercise <-
speeches_clean %>%
  mutate(
    text_opponent_masked = case_when(
      candidate == "BIDEN" ~ str_replace_all(
        text,
        regex("donald trump|trump|donald", ignore_case = TRUE),
        "the_other_guy"
      ),
      candidate == "TRUMP" ~ str_replace_all(
        text,
        regex("joe biden|biden|joe", ignore_case = TRUE),
        "the_other_guy"
      ),
      TRUE ~ text
    ),
    trump_post = ifelse(candidate =="TRUMP" & post_election == 1, 1,0)
  )

```

##### Quick check. how much do they talk about each other? 

```{r}
plot_word_mentions("the_other_guy",text_column=text_opponent_masked, speeches_exercise)

```

### Now we can dig on it: What are similarities and differences in how they talk about each other?

## Load pre-trained word embedding model

##Load pre-trained word embedding model and the transformation matrix

```{r}

# Load it
word_vectors <- readRDS("data/glove.rds")
transform_matrix <- readRDS("data/khodakA.rds")

```

##### We have to tokenize with this new db, such that our label is included

```{r}

#' Tokenizing with Quanteda -----------------------------------------------------------

# create corpus
corpus <- quanteda::corpus(speeches_exercise, docid_field="speech_ID",text_field="text_opponent_masked")

# tokenize corpus removing unnecessary (i.e. semantically uninformative) elements
toks <- tokens(corpus, remove_punct=T, remove_symbols=T, remove_numbers=T, remove_separators=T)

# clean out stopwords and words with 2 or fewer characters
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove", min_nchar=3)

# only use features that appear at least 5 times in the corpus
feats <- dfm(toks_nostop, tolower=T, verbose = FALSE) %>% dfm_trim(min_termfreq = 5) %>% featnames()

# leave the pads so that non-adjacent words will not become adjacent
toks_nostop_feats <- tokens_select(toks_nostop, feats, padding = TRUE)
```


```{r}
pattern <- c("the_other_guy")
# build a tokenized corpus of contexts surrounding the target term we want
term_toks <- tokens_context(x = toks_nostop_feats, pattern = pattern, window = 6L)

```

##### Q: Why does this work?

```{r}
head(docvars(term_toks), 3)

```

```{r}
# build a document-feature matrix
term_dfm <- dfm(term_toks)
head(term_dfm)

```



```{r}
# build the document-embedding-matrix
term_dem <- dem(x = term_dfm, pre_trained = word_vectors, transform = TRUE, transform_matrix = transform_matrix, verbose = TRUE)

```




```{r}
# to get a single "corpus-wide" embedding of the term(s) we are interested in, take the column average
term_wv <- matrix(colMeans(term_dem), ncol = ncol(term_dem)) %>%  `rownames<-`(docvars(term_toks)[1,1])
dim(term_wv)

```



```{r}
# to get candidate-specific embeddings for the term(s) we are interested in, average within candidate
term_wv_candidate <- dem_group(term_dem, groups = term_dem@docvars$candidate)
dim(term_wv_candidate)

```

```{r}
# find nearest neighbors by candidate
# setting as_list = FALSE combines each group's results into a single tibble (useful for joint plotting)
term_nns <- nns(term_wv_candidate, pre_trained = word_vectors, N = 5, candidates = term_wv_candidate@features, as_list = TRUE)

# check out results for Trump
term_nns[["TRUMP"]]
# check out results for Biden
term_nns[["BIDEN"]]
```




```{r}
# compute the cosine similarity between each candidate's embedding and a specific set of words
conText::cos_sim(term_wv_candidate, pre_trained = word_vectors, features = c('danger', 'corrupt','sleepy'), as_list = FALSE)

```

```{r}
# Let's get the ratio of nearest neighbors 

# we limit candidates to features in our corpus
feats <- featnames(dfm(term_toks))

# compute ratio
set.seed(12345678)
terms_nns_ratio <- get_nns_ratio(x = term_toks, 
              N = 15,
              groups = docvars(term_toks, 'candidate'),
              numerator = "TRUMP",
              candidates = feats,
              pre_trained = word_vectors,
              transform = TRUE,
              transform_matrix = transform_matrix,
              bootstrap = TRUE,
              num_bootstraps = 100,
              permute = TRUE,
              num_permutations = 100,
              verbose = FALSE)
```



```{r}
terms_nns_ratio
```


```{r}
plot_nns_ratio(x = terms_nns_ratio, alpha = 0.05, horizontal = TRUE)
```



```{r}
# two factor covariates
set.seed(12345678)
model1 <- conText(formula = the_other_guy ~ candidate + post_election,
                  data = toks_nostop_feats,
                  pre_trained = word_vectors,
                  transform = TRUE, transform_matrix = transform_matrix,
                  confidence_level = 0.95,
                  permute = TRUE, num_permutations = 100,
                  window = 6, case_insensitive = TRUE,
                  verbose = FALSE)

```

```{r}


BPre_wv <- model1['(Intercept)',] # Biden pre Election
BPost_wv <- model1['(Intercept)',] + model1['post_election',] # Biden post Election
TPre_wv <- model1['(Intercept)',] + model1['candidate_TRUMP',] # Trump pre Election
TPost_wv <- model1['(Intercept)',] + model1['candidate_TRUMP',] + model1['post_election',] # Trump post Election

# nearest neighbors
nns(rbind(BPre_wv,BPost_wv,TPre_wv,TPost_wv), N = 10, pre_trained = word_vectors, candidates = model1@features)
```
