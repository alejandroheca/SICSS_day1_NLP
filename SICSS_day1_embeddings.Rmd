---
title: "SICSS Day 1: NLP"
subtitle: "Having fun with word embeddings"
output:
  html_document:
    code_folding: hide
date: "`r format(Sys.time(), '%d %B, %Y')`"

author: "Alejandro Hermida Carrillo"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Setup
### Load the Necessary Packages
```{r, include=FALSE}

# Expression that loads a package by name
req <- substitute(require(x, character.only = TRUE))
# List of libraries
libs <- c("data.table", "ggplot2","tidyverse","conText","quanteda")
# Attempt to load each package; if not installed, install it and try again
sapply(libs, function(x) eval(req) || {install.packages(x, dependencies = TRUE); eval(req)})

```

### Set Working Directory and Load Helpers

```{r, include=FALSE}
# Set working directory    -----------------------------------------------------
path <- dirname(rstudioapi::getActiveDocumentContext()$path) # This is a relative path. Set alternative path if necessary
setwd(path)
getwd()

#Load the helper functions to work with simple emb§edding computations

source("helpers/SICSS_day1_helpers_functions.R")

```


###Part 1: understanding embedding models

### Load GloVe: a model pre-trained on Wikipedia (400k vocabulary)

```{r, include=FALSE}

glove_path <- "data/glove.rds"

glove_url <- "https://www.dropbox.com/scl/fi/22mmhoj9qctijrityy2fz/glove.rds?rlkey=t9lwh1gidt55p4zisp0ouxiut&st=we2lcaok&dl=1"
# Load it
# Check if file exists, otherwise download it
if (!file.exists(glove_path)) {
  message("glove.rds not found — downloading from Dropbox...")
  dir.create("data", showWarnings = FALSE)
  download.file(glove_url, destfile = glove_path, mode = "wb")
} else {
  message("glove.rds already present locally.")
}

# Load into object
word_vectors <- readRDS(glove_path)
```

# How does a word embedding model look like?

```{r}
View(head(word_vectors,20))
```

## Property 1. Embedding models group more similar words near from each other

### Example: Most Similar to "king"

```{r}
most_similar("king", word_vectors, 10)
```

### And most dissimilar?

```{r}
most_similar("king", word_vectors, 10,direction = "least")
```

## Plot some words

```{r}

plot_words(c("king", "queen", "man", "woman", "child", "prince", "princess"), word_vectors)

```

### What can we tell from this simple plot?

## T: Now decide on a new word with the person to your right and add it to the list. 
### What happens to the plot? Does it make sense?

```{r}

plot_words(c("king", "queen", "man", "woman", "child", "prince", "princess"), word_vectors)

```

## Property 2. An embedding model can solve analogies

```{r}

analogy("king", "man","woman", word_vectors, 5)

```

## Q: How does this work? 

### So far so good. But why do we care?
### While wrong and noisy, these simple models capture some signal on the cognitive storing of words 

## T: Think of another analogy with the classmate to your left and test it! 

```{r}

analogy("circus","clown",  "professor", word_vectors, 5)

```

#### Do the results always make sense?


## End of part 1

### Part 2: Understanding differences in meaning

#### We load our (clean) speeches dataset

```{r}

speeches_clean <- readRDS("data/speeches_clean.rds")

```


##### Now we break each speech to into tokens (words). We use the unique speech identifier included in the DB

```{r}

#' Tokenizing with Quanteda -----------------------------------------------------------

# create corpus. We need to provide the unique identifier of each speech and the column where the text to be tokenized is stored
# The meta data will be maintained
corpus <- quanteda::corpus(speeches_clean, docid_field="speech_ID",text_field="text")

# tokenize corpus removing unnecessary (i.e. semantically uninformative) elements
toks <- tokens(corpus, remove_punct=T, remove_symbols=T, remove_numbers=T, remove_separators=T)

# clean out stopwords and words with 2 or fewer characters
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove", min_nchar=3)

# only use features that appear at least 5 times in the corpus
feats <- dfm(toks_nostop, tolower=T, verbose = FALSE) %>% dfm_trim(min_termfreq = 5) %>% featnames()

# leave the pads so that non-adjacent words will not become adjacent
toks_nostop_feats <- tokens_select(toks_nostop, feats, padding = TRUE)

```

#### The steps above are standard pre-processing for open vocabulary aanalyses (e.g., topic modelling). 

#### conText starts below We select a given word (or words) and we extract the words surrounding it

```{r}
# I am choosing free / freedom, but can be anything
pattern <- c("freed*","free")

# build a tokenized corpus of contexts surrounding the target term we want
term_toks <- tokens_context(x = toks_nostop_feats, pattern = pattern, window = 6L)

```



#### Quick check. Who uses the word more?

```{r}
# Let's use the function plot_word_mentions from the helpers file
plot_word_mentions("freedom",text_column=text, speeches_clean)

```


```{r}
# build a document-feature matrix
term_dfm <- dfm(term_toks)

head(term_dfm)

```
### The next step is where conText does its job: it obtains a unique embedding (300 dims.) for each use of each unique word

#### For this, it requires a matrix of weights that ensures that irrelevant words are less influential in the output

```{r}

# Load it
transform_matrix <- readRDS("data/khodakA.rds")

```

#### Then it extracts the 

```{r}
# build the document-embedding-matrix
term_dem <- dem(x = term_dfm, pre_trained = word_vectors, transform = TRUE, transform_matrix = transform_matrix, verbose = TRUE)

```


```{r}
# to get a single "corpus-wide" embedding of the term we are interested in, take the column average
term_wv <- matrix(colMeans(term_dem), ncol = ncol(term_dem)) %>%  `rownames<-`(docvars(term_toks)[1,1])
dim(term_wv)

```

```{r}
# to get candidate-specific embeddings of the term, average within candidate
term_wv_candidate <- dem_group(term_dem, groups = term_dem@docvars$candidate)
dim(term_wv_candidate)

```


#### What are the most associated words to freedom in the language of each candidate?

```{r}
# find nearest neighbors by candidate
# setting as_list = FALSE combines each group's results into a single tibble (useful for joint plotting)
term_nns <- nns(term_wv_candidate, pre_trained = word_vectors, N = 5, candidates = term_wv_candidate@features, as_list = TRUE)

# check out results for Trump
term_nns[["TRUMP"]]
# check out results for Biden
term_nns[["BIDEN"]]
```

##### How similar is their use of the words to any words we might care about?

```{r}
# compute the cosine similarity between each party's embedding and a specific set of words
conText::cos_sim(term_wv_candidate, pre_trained = word_vectors, features = c('capitalism', 'america'), as_list = FALSE)

```

##### For each candidate, let's obtain statistical estimates of the nearest neighbours in their use of freedom 

```{r}
# we limit candidates to features in our corpus
feats <- featnames(dfm(term_toks))

# compare nearest neighbors between groups
set.seed(12345678) 
term_candidate_nns <- get_nns(x = term_toks, N = 10,
        groups = docvars(term_toks, 'candidate'),
        candidates = feats,
        pre_trained = word_vectors,
        transform = TRUE,
        transform_matrix = transform_matrix,
        bootstrap = TRUE,
        num_bootstraps = 100, 
        confidence_level = 0.95,
        as_list = TRUE)
```

```{r}
term_candidate_nns[["TRUMP"]]
term_candidate_nns[["BIDEN"]]

```
##### That's not super informative, many repeated words. The ratio tells us which words are mostly used by each of them, and which ones are shared

```{r}
# compute the cosine similarity between each candidate's embedding of the chosen term and a specific set of features. 
# Here: the features found in the context 
nns_ratio(x = term_wv_candidate, N = 10, numerator = "TRUMP", candidates = term_wv_candidate@features, pre_trained = word_vectors, verbose = FALSE)

```

##### This looks more meaningful. Now let's include a statistical test

```{r}
# we limit candidates to features in our corpus
feats <- featnames(dfm(term_toks))

# compute ratio
set.seed(12345678)
terms_nns_ratio <- get_nns_ratio(x = term_toks, 
              N = 15,
              groups = docvars(term_toks, 'candidate'),
              numerator = "TRUMP",
              candidates = feats,
              pre_trained = word_vectors,
              transform = TRUE,
              transform_matrix = transform_matrix,
              bootstrap = TRUE,
              num_bootstraps = 100,
              permute = TRUE,
              num_permutations = 100,
              verbose = FALSE)
```



```{r}
terms_nns_ratio
```


```{r}
plot_nns_ratio(x = terms_nns_ratio, alpha = 0.05, horizontal = TRUE)
```

##### Embedding regression: include confounders
###### E.g., both the frecuency of use of the terms by each candidate as well as the meaning might be influences by the covid pandemic
```{r}
# two factor covariates
set.seed(12345678)
model1 <- conText(formula = c("freed*","free") ~ candidate + covid_period,
                  data = toks_nostop_feats,
                  pre_trained = word_vectors,
                  transform = TRUE, transform_matrix = transform_matrix,
                  confidence_level = 0.95,
                  permute = TRUE, num_permutations = 100,
                  window = 6, case_insensitive = TRUE,
                  verbose = FALSE)


```


##### We can interrogate the estimates to obtain embeddings for each 

```{r}

BPost_wv <- model1['(Intercept)',] # Biden no COVID
BPre_wv <- model1['(Intercept)',] + model1['covid_period',] # Biden and COVID 
TPost_wv <- model1['(Intercept)',] + model1['candidate_TRUMP',] # Trump no COVID
TPre_wv <- model1['(Intercept)',] + model1['candidate_TRUMP',] + model1['covid_period',] # Trump and COVID

# nearest neighbors
nns(rbind(BPre_wv,BPost_wv,TPre_wv,TPost_wv), N = 10, pre_trained = word_vectors, candidates = model1@features)

```

### Let's try a different word! What could they use in different ways?


